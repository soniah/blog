<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>High Availability on Sonia Hamilton - Blog</title>
    <link>http://blog2.snowfrog.net/tags/high-availability/</link>
    <description>Recent content in High Availability on Sonia Hamilton - Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 14 Jan 2011 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://blog2.snowfrog.net/tags/high-availability/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The RAID5 Write Hole</title>
      <link>http://blog2.snowfrog.net/2011/01/14/the-raid5-write-hole/</link>
      <pubDate>Fri, 14 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>http://blog2.snowfrog.net/2011/01/14/the-raid5-write-hole/</guid>
      <description>&lt;p&gt;The latest edition of the venerable &lt;a href=&#34;http://www.amazon.com/UNIX-Linux-System-Administration-Handbook/dp/0131480057/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1294981909&amp;amp;sr=1-1&#34;&gt;UNIX and Linux System Administration Handbook&lt;/a&gt; (Nemeth et al) has a good section discussing the &amp;ldquo;RAID5 Write Hole&amp;rdquo;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Finally, RAID 5 is vulnerable to corruption in certain circumstances. Its incremental updating of parity data is more efficient than reading the entire stripe and recalculating the stripeâ€™s parity based on the original data. On the other hand, it means that at no point is parity data ever validated or recalculated. If any block in a stripe should fall out of sync with the parity block, that fact will never become evident in normal use; reads of the data blocks will still return the correct data.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;**&lt;em&gt;Only when a disk fails does the problem become apparent. The parity block will likely have been rewritten many times since the occurrence of the original desynchronization. Therefore, the reconstructed data block on the replacement disk will consist of essentially random data.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Further reading on the &lt;a href=&#34;http://www.miracleas.com/BAARF/BAARF2.html&#34;&gt;BAARF&lt;/a&gt; archive (Battle Against Any Raid 5), including why RAID10 and RAID3 should be chosen over RAID5. And then there&amp;rsquo;s &lt;a href=&#34;http://blogs.sun.com/bonwick/entry/raid_z&#34;&gt;ZFS and RAID-Z&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Troubleshooting Linux HA (High Availability)</title>
      <link>http://blog2.snowfrog.net/2009/02/25/troubleshooting-linux-ha-high-availability/</link>
      <pubDate>Wed, 25 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>http://blog2.snowfrog.net/2009/02/25/troubleshooting-linux-ha-high-availability/</guid>
      <description>&lt;p&gt;When Linux HA (High Availability) is setup, each machine will have a physical address, and one machine should also have the virtual address.&lt;/p&gt;

&lt;p&gt;This can be checked via &lt;strong&gt;ip addr&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;machine 1
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc pfifo_fast qlen 1000
    link/ether 00:09:3d:12:af:77 brd ff:ff:ff:ff:ff:ff
    inet 999.99.133.12/23 brd 211.29.133.255 scope global eth0

machine 2
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc pfifo_fast qlen 1000
    link/ether 00:09:3d:12:ba:ef brd ff:ff:ff:ff:ff:ff
    inet 999.99.133.13/23 brd 211.29.133.255 scope global eth0
    inet 999.99.133.19/23 brd 211.29.133.255 scope global secondary eth0:1&lt;/pre&gt;

&lt;p&gt;If this isn&amp;rsquo;t the case, do a &lt;strong&gt;hb_takeover&lt;/strong&gt; on the appropriate machine (depending on the status of the underlying application). Eg &lt;strong&gt;/usr/lib64/heartbeat/hb_takeover&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>